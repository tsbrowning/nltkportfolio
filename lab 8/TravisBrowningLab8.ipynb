{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travis Browning\n",
    "7LN001\n",
    "Portfolio 6, Week 8 practical session\n",
    "9 February, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Write an automatic summariser based on word frequencies. Do the following steps: \n",
    "\n",
    "a) Read the contents of a file and split it into sentences and each sentence into tokens. If you wantyou can hard code the name of the file in your program or youcan ask the user to enter it.\n",
    "\n",
    "b) Calculate the frequency of each token, ignoring stopwords\n",
    "\n",
    "c) Calculate the score of each sentence by adding the scores of the tokens it contains and normalise it by the number of tokens in the sentence\n",
    "\n",
    "d) Extract the n sentences (n specified by the user) with the highest scores and present them in theorderthey occur in the text\n",
    "\n",
    "Use as much as you can NLTK. For stopwords check 4.1 Wordlist Corpora from http://www.nltk.org/book/ch02.html \n",
    "\n",
    "You can read more about automatic summarisation at: https://en.wikipedia.org/wiki/Automatic_summarization \n",
    "\n",
    "You do not need to process English texts. The method above is language independent, so you can process texts in other languages as long as you have the relevant resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example0: a look at the skateboard drought of 2020\n",
    "example1: personal reference letter\n",
    "example2: Herman Meville - Moby Dick\n",
    "example3: Vancouver Canucks eliminate St Louis Blues in game 6\n",
    "example4: Home schooling troubles during Covid 19\n",
    "example5: From Brown corpus, mystery\n",
    "example6: Gordon Lightfoot - The Wreck of the Edmund Fitzgerald\n",
    "example7: Wikipedia - Voiceless dental fricative\n",
    "example8: Blake poems\n",
    "example9: Thomas Jefferson Autobiography\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter an example to summarise: 2\n",
      "Press 0 to view sentences ranked by content word score, or press 1 to view the frequency of tokens: 1\n",
      "\n",
      "Below are up to 100 of the most frequent content words in the example:\n",
      " \n",
      "('whale', 1226)\n",
      "('one', 921)\n",
      "('like', 647)\n",
      "('upon', 566)\n",
      "('man', 527)\n",
      "('ship', 518)\n",
      "('ahab', 511)\n",
      "('sea', 455)\n",
      "('old', 450)\n",
      "('would', 432)\n",
      "('though', 384)\n",
      "('yet', 345)\n",
      "('head', 345)\n",
      "('boat', 336)\n",
      "('time', 334)\n",
      "('long', 333)\n",
      "('captain', 329)\n",
      "('still', 312)\n",
      "('great', 306)\n",
      "('said', 304)\n",
      "('two', 298)\n",
      "('seemed', 283)\n",
      "('must', 283)\n",
      "('white', 281)\n",
      "('last', 277)\n",
      "('see', 272)\n",
      "('way', 271)\n",
      "('thou', 271)\n",
      "('whales', 268)\n",
      "('stubb', 257)\n",
      "('queequeg', 252)\n",
      "('little', 249)\n",
      "('round', 247)\n",
      "('three', 245)\n",
      "('sperm', 244)\n",
      "('men', 244)\n",
      "('say', 244)\n",
      "('may', 240)\n",
      "('first', 235)\n",
      "('every', 232)\n",
      "('well', 230)\n",
      "('much', 223)\n",
      "('good', 216)\n",
      "('could', 216)\n",
      "('hand', 214)\n",
      "('side', 208)\n",
      "('never', 206)\n",
      "('ever', 206)\n",
      "('look', 205)\n",
      "('starbuck', 198)\n",
      "('deck', 196)\n",
      "('almost', 195)\n",
      "('even', 191)\n",
      "('water', 190)\n",
      "('thing', 188)\n",
      "('away', 183)\n",
      "('might', 183)\n",
      "('come', 179)\n",
      "('made', 178)\n",
      "('day', 176)\n",
      "('world', 176)\n",
      "('sir', 175)\n",
      "('life', 174)\n",
      "('pequod', 173)\n",
      "('chapter', 173)\n",
      "('fish', 169)\n",
      "('among', 167)\n",
      "('many', 166)\n",
      "('far', 165)\n",
      "('seen', 165)\n",
      "('back', 164)\n",
      "('line', 158)\n",
      "('let', 158)\n",
      "('without', 156)\n",
      "('eyes', 156)\n",
      "('aye', 155)\n",
      "('cried', 155)\n",
      "('know', 152)\n",
      "('sort', 152)\n",
      "('god', 152)\n",
      "('right', 151)\n",
      "('night', 150)\n",
      "('thought', 150)\n",
      "('part', 149)\n",
      "('boats', 147)\n",
      "('air', 143)\n",
      "('crew', 140)\n",
      "('whole', 137)\n",
      "('take', 137)\n",
      "('half', 136)\n",
      "('tell', 135)\n",
      "('thus', 133)\n",
      "('things', 132)\n",
      "('whaling', 131)\n",
      "('thee', 131)\n",
      "('hands', 130)\n",
      "('soon', 130)\n",
      "('came', 130)\n",
      "('mast', 129)\n",
      "\n",
      " End of tokens\n",
      "Continue? y\n",
      "Please enter an example to summarise: 3\n",
      "Press 0 to view sentences ranked by content word score, or press 1 to view the frequency of tokens: 20\n",
      "Invalid entry.\n",
      "Please enter an example to summarise: 3\n",
      "Press 0 to view sentences ranked by content word score, or press 1 to view the frequency of tokens: 0\n",
      "How many sentences would you like to view? 20\n",
      "\n",
      "Sentence number, Sentence text, Word length, Content word length, Content word length/Word length\n",
      "[1, 'Jacob Markstrom made 34 saves, and the Vancouver Canucks eliminated the defending champion St. Louis Blues from the Stanley Cup Playoffs with a 6-2 win in Game 6 of the Western Conference First Round series at Rogers Place in Edmonton on Friday.', 42, 29, 0.6905]\n",
      "[4, '1 seed Vegas Golden Knights in the Western Conference Second Round.', 11, 9, 0.8182]\n",
      "[6, 'ET; NBCSN, CBC, SN, TVAS).', 5, 5, 1.0]\n",
      "[7, '\"Obviously, it\\'s a great feeling,\" Canucks forward Jay Beagle said.', 10, 8, 0.8]\n",
      "[8, '\"That\\'s why we\\'re here.', 4, 3, 0.75]\n",
      "[15, \"It's so much fun to play hockey right now.\", 9, 6, 0.6667]\n",
      "[18, '4 seed.', 2, 2, 1.0]\n",
      "[21, '\"The decision, really, Jake played three in a row and obviously lost the third one, and [Binnington] has been a big-time goalie for us for a long time,\" Blues coach Craig Berube said.', 33, 22, 0.6667]\n",
      "[33, '...', 1, 1, 1.0]\n",
      "[36, '\"That\\'s a great team we beat over there,\" Canucks coach Travis Green said.', 13, 10, 0.7692]\n",
      "[41, '\"You could see they were hyped up for this series,\" Blues captain Alex Pietrangelo said.', 15, 10, 0.6667]\n",
      "[42, '\"They\\'re a good team.', 4, 3, 0.75]\n",
      "[43, \"They've got good young players, veteran guys who really contribute, and they played us well.\", 15, 12, 0.8]\n",
      "[45, \"They'll have success.\", 3, 2, 0.6667]\n",
      "[48, '\"It didn\\'t seem like our energy was coming from everyone,\" Blues forward David Perron said.', 15, 11, 0.7333]\n",
      "[49, '\"It was tough.', 3, 2, 0.6667]\n",
      "[51, \"Honestly, I can't even think about this whole thing right now.\", 11, 8, 0.7273]\n",
      "[55, '\"Give them credit, they played smart.', 6, 4, 0.6667]\n",
      "[58, 'NOTES: Canucks forward Elias Pettersson had two assists and has 13 points (four goals, nine assists) in 10 playoff games, tied with Nathan MacKinnon of the Colorado Avalanche for the NHL lead.', 32, 23, 0.7188]\n",
      "[59, 'â€¦ Vancouver defenseman Quinn Hughes had an assist to become the fourth rookie defenseman to get 10 points (one goal, nine assists) in 10 postseason games or fewer.', 28, 21, 0.75]\n",
      "Continue? n\n",
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "#importing nltk, re and collections for counting mechanism\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "import collections\n",
    "\n",
    "while True:\n",
    "    exIn = input(\"Please enter an example to summarise: \")\n",
    "    choice = int(input(\"Press 0 to view sentences ranked by content word score, or press 1 to view the frequency of tokens: \"))\n",
    "    if choice == 0:\n",
    "    \n",
    "        num = input(\"How many sentences would you like to view? \")\n",
    "        n = int(num)\n",
    "        print(\"\\nSentence number, Sentence text, Word length, Content word length, Content word length/Word length\")\n",
    "        \n",
    "    elif choice != 1:\n",
    "        print('Invalid entry.')\n",
    "        continue\n",
    "\n",
    "\n",
    "    #function to define and remove stopwords\n",
    "    def contentList(text):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        content = [w for w in text if w not in stopwords]\n",
    "        return(content)\n",
    "\n",
    "    '''\n",
    "    a) open example, split example into lists of sentences that are lists of tokens, and then filter stopwords using function\n",
    "    '''\n",
    "\n",
    "    frequency = {}\n",
    "    preList = []\n",
    "\n",
    "    #open example\n",
    "    for ex in exIn:\n",
    "        \n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "        #defining examples from NLTK or from local directory\n",
    "        #corpus sentences are printing as lists, so they're joined in s1\n",
    "        if ex == '2':\n",
    "            lines = []\n",
    "            l1 = gutenberg.sents('melville-moby_dick.txt')\n",
    "            for s in l1:\n",
    "                s1 = ' '.join(s)\n",
    "                lines.append(s1)\n",
    "                \n",
    "        elif ex == '5':\n",
    "            lines = []\n",
    "            l1 = brown.sents(categories = 'mystery')\n",
    "            for s in l1:\n",
    "                s1 = ' '.join(s)\n",
    "                lines.append(s1)\n",
    "                \n",
    "        elif ex == '8':\n",
    "            lines = []\n",
    "            l1 = gutenberg.sents('blake-poems.txt')\n",
    "            for s in l1:\n",
    "                s1 = ' '.join(s)\n",
    "                lines.append(s1)\n",
    "        else:\n",
    "            file = open(\"example\" + exIn + \".txt\", \"r\")\n",
    "            opFile= file.read()\n",
    "            \n",
    "            #using pickle to tokenize sentences for examples that don't come from corpora\n",
    "\n",
    "            lines = tokenizer.tokenize(opFile)      \n",
    "    \n",
    "        sentInd = 0\n",
    "        \n",
    "    \n",
    "        for sent in lines:\n",
    "            #lower case for processing\n",
    "            sentInd = sentInd + 1\n",
    "            sentLen = len(sent.split())\n",
    "            lowerSent = sent.lower()\n",
    "            tokens = lowerSent.split()\n",
    "            #remove stopwords\n",
    "            preProcess = contentList(tokens)\n",
    "            contentLen = len(preProcess)\n",
    "        \n",
    "            '''\n",
    "            c) Calculate the score of each sentence by adding the scores of the tokens it \n",
    "            contains and normalise it by the number of tokens in the sentence\n",
    "            '''\n",
    "        \n",
    "            score1 = contentLen / sentLen\n",
    "            score = round(score1, 4)\n",
    "        \n",
    "            #regex defining pattern for counting\n",
    "            match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', str(preProcess))\n",
    "    \n",
    "            #define keys, and counting mechanism\n",
    "            for word in match_pattern:\n",
    "                count = frequency.get(word,0)\n",
    "                frequency[word] = count + 1    \n",
    "            frequency_list = frequency.keys()\n",
    "        \n",
    "            sentOut = [sentInd, sent, sentLen, contentLen, score]\n",
    "\n",
    "            preList.append(sentOut)\n",
    "            #sort sentences by content words/ words  \n",
    "            def scoreSort (subList):\n",
    "                return(sorted(subList, key=lambda x: x[4]))\n",
    "        \n",
    "            preScore = (scoreSort(preList))\n",
    "\n",
    "        #sort tuples by second parameter, for b)\n",
    "        def sort(subList):\n",
    "            return(sorted(subList, key = lambda x: x[1]))\n",
    "        \n",
    "        freqList = []\n",
    "        \n",
    "        \n",
    "        for words in frequency_list:\n",
    "            p = words, frequency[words]\n",
    "            freqList.append(p)\n",
    "    \n",
    "        #reversed to display from high to low\n",
    "        printList = reversed(sort(freqList))\n",
    "    \n",
    "\n",
    "    \n",
    "        '''\n",
    "        b) b) Calculate the frequency of each token, ignoring stopwords\n",
    "        '''\n",
    "    \n",
    "        if choice == 1:\n",
    "            pOut0 = []\n",
    "            print(\"\\nBelow are up to 100 of the most frequent content words in the example:\\n \")\n",
    "            \n",
    "        \n",
    "            for token in printList:\n",
    "                pOut0.append(token)\n",
    "                \n",
    "            #limiting the output to 100 iterations, as the examples from the corpora are large   \n",
    "            pOut1 = pOut0 [:99]\n",
    "            for t in pOut1:\n",
    "                print(t)\n",
    "                \n",
    "        \n",
    "            print('\\n End of tokens')\n",
    "        \n",
    "    \n",
    "        scoreOut = preScore [::-1]\n",
    "    \n",
    "        if choice == 0:\n",
    "            prePrint = scoreOut[:n]\n",
    "            \n",
    "            #Sort sentence lists by first value\n",
    "            def printSort (subList):\n",
    "                return(sorted(subList, key=lambda x: x[0]))\n",
    "            \n",
    "            #as formatted above in sentOut, first value is sentence number\n",
    "            finPrint = (printSort(prePrint))\n",
    "            \n",
    "            for row in finPrint:\n",
    "                print(row)\n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "        if exIn != '2':\n",
    "            if exIn != '5':\n",
    "                if exIn != '8':\n",
    "                    file.close()\n",
    "        \n",
    "    c = input(\"Continue? \")\n",
    "    c1 = c.lower()\n",
    "    if c1[0] == 'n':\n",
    "        print('Goodbye')\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
